# -*- coding: utf-8 -*-
"""Copy of MIC Workshop 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VrerXA5aJ_dABx61ilWP7m7wS39qffdh

# MIC Workshop 2: Optimization & Gradient-Based Learning

## Background
Hope you're enjoying the second Deep Learning workshop so far! Today, we'll be going over Gradient Descent and Backpropogation and test out our skills on a new dataset (CIFAR-10). This is a common lesson in ML courses, as backpropogation and gradient descent form the backbone of most learning algorithms.

## Feedback Survey 

**At the end of the workshop, please fill out the feedback survey [HERE](https://goo.gl/rXV5EQ) for suggestions on how we can improve our future workshops! **

## Installing PyTorch
Don't worry too much about the contents of this cell. It basically just installs the right packages for you to run PyTorch code

If this cell is causing problems for you (like `tcmalloc`,  make sure you click "connect to Hosted runtime" from the dropdown menu in the top right)
"""

'''
# Installing pytorch, don't worry about the code in this cell. 
# http://pytorch.org/
from os.path import exists
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
accelerator = 'cpu' #cuda_output[0] if exists('/dev/nvidia0')

# !pip install torch torchvision
!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision
'''


"""## Imports"""

import torch, os
from tqdm import tqdm as tqdm
from torchvision.datasets import CIFAR10 
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

"""# The Data
For this lab, we will use a popular image dataset for machine learning - [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). It consists of 60000 color images in 10 classes and is a commonly used benchmark in academic literature. Because of its popularity, it's already in Torch; we just need to import it

You don't need to worry about the format too much now. But in general, getting the data in the necessary format is usually a key (albeit mundane) part of the process.
"""

## load cifar dataset

root = './data'
if not os.path.exists(root):
  os.mkdir(root)

# In addition to transforming the image into a tensor, we also normalize the values in the image
# so that the mean pixel value is subtracted and divided by the pixel standard deviation.
imgTransform = transforms.Compose([transforms.ToTensor(),
                                   transforms.Normalize((0.4914, 0.4822, 0.4465), 
                                                        (0.2023, 0.1994, 0.2010)),
                                   transforms.Lambda(lambda inputs: inputs.view(3 * 32 * 32))])
trainset = CIFAR10(root='./data', train = True, transform = imgTransform, download = True)
valset = CIFAR10(root='./data', train = False, transform = imgTransform, download = True)

"""We're going to make batches of size 128 for the purpose of training. Torch has `DataLoader` classes, which are used to load batches at a time in a (somewhat) optimized fashion."""

trainLoader = torch.utils.data.DataLoader(
                trainset, batch_size = 128, 
                shuffle = True, num_workers = 0)
valLoader = torch.utils.data.DataLoader(
                valset, batch_size = 128, 
                shuffle = False, num_workers = 0)

print ('total number of trainning batches: {}'.format(len(trainLoader)))
print ('total number of testing batches: {}'.format(len(valLoader)))

"""## The Loss Function
The loss function measures how well the neural network manages to reach its goal of generating outputs as close as possible to the desired values. 

For this task, since we are doing classification, we will implement and use Cross Entropy loss. It's the most common loss function for Classification tasks. You can read more about Cross Entropy Loss [here](https://en.wikipedia.org/wiki/Cross_entropy). 

Referencing the equation in the link above (or slide 25), fill in the blanks below. 

**To avoid variable overflow, before computing the exponential of each input value, we subtract the maximum of the input values from each value.**
"""

class nn_CrossEntropyLoss(object): 
    # Forward pass -log softmax(input_{label})
    def forward(self, inputs, labels):
        max_val = inputs.max()  # This is to avoid variable overflows.
        exp_inputs = torch.exp(inputs-max_val)
        denominators = exp_inputs.sum(1).repeat(inputs.size(1), 1).t()
        self.predictions = torch.mul(exp_inputs, 1 / denominators)
        return -self.predictions.log().gather(1, labels.view(-1, 1)).mean()
    
    # Backward pass 
    def backward(self, inputs, labels):
        grad_inputs = self.predictions.clone()
        for i in range(0, inputs.size(0)):
            grad_inputs[i][labels[i]] = grad_inputs[i][labels[i]] - 1
        return grad_inputs

"""## The Layer 

Next, we will implement our own linear layer, which will apply a linear transformation to the incoming data. Implement the forward pass.
"""

class nn_Linear(object):
    def __init__(self, inputSize, outputSize):
        self.weight = torch.Tensor(inputSize, outputSize).normal_(0, 0.01)
        self.gradWeight = torch.Tensor(inputSize, outputSize)
        self.bias = torch.Tensor(outputSize).zero_()
        self.gradBias = torch.Tensor(outputSize)
    
    # Forward pass, inputs is a matrix of size batchSize x inputSize
    def forward(self, inputs):
        return torch.matmul(inputs,self.weight) + self.bias
    
    # Backward pass, in addition to compute gradients for the weight and bias.
    # It has to compute gradients with respect to inputs. 
    def backward(self, inputs, gradOutput):
        self.gradWeight = torch.matmul(inputs.t(), gradOutput)
        self.gradBias = gradOutput.sum(0)
        return torch.matmul(gradOutput, self.weight.t())

"""## The Activation Function 

Activation functions add non-linearity to the output of a neuron and help us define if it has been "activated" or not. Here, we implement ReLu, one of the most widely-used activation functions due to its simplicity and ease of computation. 

Reference slide 18 for the definition and fill in the blanks below.
"""

class nn_ReLU(object):
    # pytorch has an element-wise max function.
    def forward(self, inputs):
        outputs = inputs.clone()
        outputs[outputs < 0] = 0
        return outputs
    
    # Make sure the backward pass is absolutely clear.
    def backward(self, inputs, gradOutput):
        gradInputs = gradOutput.clone()
        gradInputs[inputs < 0] = 0
        return gradInputs

"""## The Network 

Using the classes we've defined above, we first define our neural network! Using your understanding of neural networks, fill in the dimensions below. 

Note: Although we are doing an image classification task like last workshop, we are now working with color images. Be careful when you are defining your input dimensions!
"""

learningRate = 1e-4  # Single learning rate for this lab.
hidden_dims1 = 1024 
hidden_dims2 = 256 

# Definition of our network.
linear1 = nn_Linear(3072, hidden_dims1)
relu1 = nn_ReLU()
linear2 = nn_Linear(hidden_dims1, hidden_dims2) 
relu2 = nn_ReLU() 
linear3 = nn_Linear(hidden_dims2, 10) 
criterion = nn_CrossEntropyLoss()

"""## The Training
The last step of our training pipeline is the training itself! Use your understanding of the learning process of neural networks to fill in the blanks below. 

Recall the update equation:

`weight := weight - learning_rate * gradient_of_weight`

Note: For the sake of time, we will be training for only 1 epoch, but try varying the number once you're finished to see how that affects performance.
"""

num_epochs = 1 
iteration = 0

loss_list = []
iteration_list = []
accuracy_list = []


# Training loop.
for epoch in range(num_epochs):

    correct = 0.0
    cum_loss = 0.0
    counter = 0
    
    # Make a pass over the training data.
    t = tqdm(trainLoader, desc = 'Training epoch %d' % epoch)
    for (i, data) in enumerate(t):
        # Get the inputs 
        inputs, labels = data
    
        # Forward pass:
        a = linear1.forward(inputs)
        b = relu1.forward(a)
        c = linear2.forward(b)
        d = relu2.forward(c)
        e = linear3.forward(d)
        cum_loss += criterion.forward(e, labels)
        max_scores, max_labels = e.max(1)
        correct += (max_labels == labels).sum()
        
        # Backward pass:
        grads_e = criterion.backward(e, labels)
        grads_d = linear3.backward(d, grads_e)
        grads_c = relu2.backward(c, grads_d)
        grads_b = linear2.backward(b, grads_c)
        grads_a = relu1.backward(a, grads_b)
        linear1.backward(inputs, grads_a)
        
        # Weight and bias updates.
        linear1.weight = linear1.weight - learningRate * linear1.gradWeight
        linear1.bias = linear1.bias - learningRate * linear1.gradBias
        linear2.weight = linear2.weight - learningRate * linear2.gradWeight
        linear2.bias = linear2.bias - learningRate * linear2.gradBias
        linear3.weight = linear3.weight - learningRate * linear3.gradWeight
        linear3.bias = linear3.bias - learningRate * linear3.gradBias
        
        # logging information.
        counter += inputs.size(0)
        t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)

        iteration += 1

        if iteration % 50 == 0: 
    
            # Make a pass over the validation data.
            correct = 0.0
            cum_loss = 0.0
            counter = 0
            total = 0
            t = tqdm(valLoader, desc = 'Validation epoch %d' % epoch)
            for (i, (inputs, labels)) in enumerate(t):
                
                # Forward pass:
                a = linear1.forward(inputs)
                b = relu1.forward(a)
                c = linear2.forward(b)
                d = relu2.forward(c)
                e = linear3.forward(d)
                cum_loss += criterion.forward(e, labels)
                max_scores, max_labels = e.max(1)
                correct += (max_labels == labels).sum()
                total += len(labels)
                
                # logging information.
                counter += inputs.size(0)
                t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)    

            current_accuracy = 100 * correct / float(total)

            # store loss and iteration 
            last_loss = (cum_loss / (1 + i))
            loss_list.append(last_loss.item())
            accuracy_list.append(current_accuracy.item())
            iteration_list.append(iteration)

# visualization loss 
plt.plot(iteration_list, loss_list)
plt.xlabel("Number of iterations")
plt.ylabel("Loss")
plt.show()

# visualization accuracy 
plt.plot(iteration_list, accuracy_list, color="red")
plt.xlabel("Number of iterations")
plt.ylabel("Accuracy")
plt.show()

"""## What's next?
We'll talk more about this kind of stuff next week, but a fully connected neural network is only one type of a neural network. When we are faced with image classification tasks, there are more appropriate architectures, such as convolutional neural networks. 

Join us for the workshop next week on Convolutional Neural Networks and Computer Vision!

## Feedback Survey 

**Don't forget to fill out the feedback survey [here](https://goo.gl/rXV5EQ) for suggestions on how we can improve our future workshops! **
"""
